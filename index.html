<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Sunil Jaiswal</title>
  
  <meta name="author" content="Sunil Jaiswal">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <style>
    #parent{
    width: 10%;
    margin: 0 auto;
    }
    </style>

  <link rel="stylesheet" type="text/css" href="stylesheet.css">
	<link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>üåê</text></svg>">
</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Sunil Jaiswal</name>
              </p>
              <p style="text-align:justify;"> I am working as a computer vision researcher at <a href="https://www.k-lens.de/">K|Lens GmbH</a> and also leading the computer 
                vision-AI R&D team focussing on real-world computer vision problems in bringing ideas into reality from research prototypes to a product. My research interest includes:</p>                
                <p style="text-align:justify;"> 
                <li><b> Low-level vision </b>: Real-world image & video super-resolution/deblurring/denoising for both single/multiview camera.
                <li><b> Geometric vision </b>: Real-world multiview and monocular depth estimation, unsupervised video depth estimation, optical flow, NERF.
                <li><b> High-level vision </b>: Real-world anamoly detection, defect detection/segmentation/classifications for both single/multiview camera. 
              </p>
                                       

              <p style="text-align:justify;">Prior to this, I obtained my Ph.D. in The Hong Kong University of Science & Technology (<a href="https://hkust.edu.hk/">HKUST</a>) in 2017. During my Ph.D., 
                I was a visiting research scholar in the VAI lab under <a href="https://www3.cs.stonybrook.edu/~mueller/">Prof Klaus Mueller</a> at Stony Brook University, USA and SUNY Korea in 2015/2016. 
                I also visited <a href="https://www.technicolor.com/">Technicolor R&D, France</a> as a research internee and awarded "Best Research and Innovation Award" of 2017. In 2012, I received an undergraduate degree from <a href="https://lnmiit.ac.in/">LNMIIT</a>, India in the ECE department, where I worked with 
                <a href="https://research.iitj.ac.in/researcher/anil-kumar-tiwari" target="_blank">Prof. Anil Tiwari</a> & visited <a href="https://cbia.fi.muni.cz/" target="_blank">CBIA</a>, 
                Czech Republic. 
              </p>
                

              <p style="text-align:center">
                <a href="mailto:sunil.jaiswal@k-lens.de">Email</a> &nbsp/&nbsp
                <a href="data/">CV</a> &nbsp/&nbsp
                <a href="https://scholar.google.com/citations?hl=en&user=dUEcwL8AAAAJ&view_op=list_works&sortby=pubdate">Google Scholar</a> &nbsp/&nbsp
                <a href="https://www.linkedin.com/in/sunil-jaiswal-6ba49518/">LinkedIN</a> 
              </p>
            </td>
            <td style="padding:2.5%;width:40%;max-width:40%">
              <a href="images/profile.jpg"><img style="width:75%;max-width:75%" alt="profile photo" src="images/profile.jpg" class="hoverZoomLink"></a>
            </td>
          </tr>
    

        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">

              <br> <br>
              <heading>Experience</heading>
              <p>
                I am fortunate enough to work with some great minds:

                <li> <b> Jan'2018 - Present </b>: Computer Vision Researcher at <a href="https://www.k-lens.de/" target="_blank">K|Lens, GmbH</a>, Germany<br/></li>
                <li> <b> Sep'2012 - Dec'2017 </b>: Ph.D. Candidate at <a href="https://www.ust.hk/" target="_blank">HKUST</a>, Hong Kong under the supervision of
                <a href="https://www.linkedin.com/in/oscar-au-59b3a411/" target="_blank">Prof. Oscar Au</a><br/></li>
                <li> <b> Apr'2015 - Mar'2016 </b>: Visiting Scholar at Stony Brook University, USA & SUNY Korea under the supervision of
                                              <a href="http://www3.cs.stonybrook.edu/~mueller/" target="_blank">Klaus Mueller</a><br/></li>
                <li> <b> Dec'2015 - Dec'2017 </b>: Worked with <a href="http://luvision.net/" target="_blank">Prof. Lu Fang</a>, Tsinghua University, China<br/></li>
                <li> <b> June'2017 - Oct'2017 </b>: Research internee at Technicolor R&D, France under the supervision of
                                              <a href="https://www.technicolor.com/" target="_blank">Dr. F. Galpin</a>,
                                              <a href="https://www.technicolor.com/" target="_blank">Dr. R. Fabien</a><br/></li>
      
                <li> <b> Jun'2011 - Aug'2011 </b>: Research internee at <a href="https://cbia.fi.muni.cz/" target="_blank">CBIA</a>, Czech Republic
                                              under the supervision of<a href="http://www.fi.muni.cz/~kozubek/" target="_blank"> M. Kozubek</a>, <a href="https://cbia.fi.muni.cz/people/" target="_blank">
                                              D. Svoboda</a> <br/></li>
      
                <li> <b> May - Aug'2010 & 2012 </b>: Reseach internee at IIT Jodhpur, INDIA under the supervision of
                <a href="https://research.iitj.ac.in/researcher/anil-kumar-tiwari" target="_blank">Prof. Anil Tiwari</a></li>

              </p>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>


        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">

              <br> <br>
              <heading>Selected Publication</heading>
              <p>
                <!-- I'm interested in computer vision, machine learning, optimization, and image processing. Much of my research is about 
                inferring the physical world (shape, motion, color, light, etc) from images. Representative papers 
                are <span class="highlight">highlighted</span>. -->
              </p>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
				


          <tr onmouseout="hypernerf_stop()" onmouseover="hypernerf_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" >
                <img src='images/cvpr.png' width="190">
              </div>
             </td>

               <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/abs/2305.02660">
                <papertitle>Expanding Synthetic Real-World Degradations for Blind Video Super Resolution</papertitle>
              </a>
              <br>
							<a>Mehran Jeelani*</a>,                                                        
<a href="https://sadbhawnathakur.github.io/">Sadbhawna Thakur*</a>,
<a href="https://people.mpi-inf.mpg.de/~ncheema/">Noshaba Cheema</a>,
<a href="https://www.researchgate.net/profile/Klaus-Illgner">Klaus Illgner</a>,
<a href="https://graphics.cg.uni-saarland.de/people/slusallek.html">Philipp Slusallek</a>,
<strong>Sunil Jaiswal</strong>	(*equal contribution)					
              <br>
              
              <em>IEEE Conference on Computer Vision and Pattern Recognition (CVPR) Workshops, 2023</em>
              <br>
              <a href="https://arxiv.org/abs/2305.02660">Paper</a>
              <p style="text-align:justify;"> This work shows how varied random degradations can contribute to learning an effective VSR model, especially for
real-world video artifacts.
             </p>
             </td>
          </tr> 


          <tr onmouseout="hypernerf_stop()" onmouseover="hypernerf_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" >
                <img src='images/cvpr_mono_depth.png' height="100" width="190">
              </div>
             </td>

               <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/abs/2305.01732">
                <papertitle>High-Resolution Synthetic RGB-D Datasets for Monocular Depth Estimation</papertitle>
              </a>
              <br>
							<a>Aakash Rajpal</a>,            
<a href="https://people.mpi-inf.mpg.de/~ncheema/">Noshaba Cheema</a>,
<a href="https://www.researchgate.net/profile/Klaus-Illgner">Klaus Illgner</a>,
<a href="https://graphics.cg.uni-saarland.de/people/slusallek.html">Philipp Slusallek</a>,
<strong>Sunil Jaiswal</strong>						
              <br>
              <em>IEEE Conference on Computer Vision and Pattern Recognition (CVPR) Workshops, 2023</em>
              <br>
              <a href="https://arxiv.org/abs/2305.01732">Paper</a>
              <p style="text-align:justify;"> In this work, we generate a high-resolution synthetic
                depth dataset (HRSD) which contains 100,000 color
                images and corresponding dense ground truth depth maps.
             </p>
             </td>
          </tr> 



          <tr onmouseout="hypernerf_stop()" onmouseover="hypernerf_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" >
                <img src='images/cvpr_segmentation.png' width="190">
              </div>
             </td>

               <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/abs/2304.08111">
                <papertitle>Leveraging Multi-view Data for Improved Detection Performance: An Industrial Use Case
                </papertitle>
              </a>
              <br>
							<a>Faranak Shamsafar</a>,            
              <strong>Sunil Jaiswal</strong>,
<a>  Benjamin Kelkel</a>,
<a>  Kireeti Bodduna</a>,
<a>  Klaus Illgner-Fehns</a>,
<strong>Sunil Jaiswal</strong>					
              <br>
              <em>IEEE Conference on Computer Vision and Pattern Recognition (CVPR) Workshops, 2023</em>
              <br>
              <a href="https://arxiv.org/abs/2304.08111">Paper</a>
              <p style="text-align:justify;"> In this work, We introduce a novel multi-view dataset with semi-automatic ground-truth data, 
                which results in significant labeling resource savings.
             </p>
             </td>
          </tr> 


          <tr onmouseout="hypernerf_stop()" onmouseover="hypernerf_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" >
                <img src='images/cvde_stereo.png' width="190">
              </div>
             </td>

               <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/abs/2305.02645">
                <papertitle>Edge-aware Consistent Stereo Video Depth Estimation</papertitle>
              </a>
              <br>
							<a>Elena Kosheleva,*</a>,                                                        
              <strong>Sunil Jaiswal*</strong>,	
              <a>Faranak Shamsafar</a>,
<a href="https://people.mpi-inf.mpg.de/~ncheema/">Noshaba Cheema</a>,
<a href="https://www.researchgate.net/profile/Klaus-Illgner">Klaus Illgner</a>,
<a href="https://graphics.cg.uni-saarland.de/people/slusallek.html">Philipp Slusallek</a>
(*equal contribution)
              <br>
              <em>Arxiv'2023</em>
              <br>
              <a href="https://arxiv.org/abs/2305.02645">Paper</a>
              <p style="text-align:justify;"> In contrast to the naive method of estimating depths from images, a more sophisticated approach uses 
                temporal information, thereby eliminating flickering and geometrical inconsistencies. We propose a consistent method for 
                dense video depth estimation; however, unlike the existing monocular methods, ours relates to stereo videos.
             </p>
             </td>
          </tr> 



          <tr onmouseout="hypernerf_stop()" onmouseover="hypernerf_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='hypernerf_image'>
                <img src='images/tmm.png' width="190">
              </div>
             </td>

               <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://ieeexplore.ieee.org/abstract/document/9891833">
                <papertitle>Context Region Identification based Quality Assessment of 3D Synthesized Views</papertitle>
              </a>
              <br>
							
              <a href="https://sadbhawnathakur.github.io/">Sadbhawna Thakur*</a>,
							<a href="https://sites.google.com/view/vinitjakhetiya/home/">Vinit jakhetiya</a>, 
<a href="https://sites.google.com/view/badrisubudhi/home?authuser=0">Badri N. Subudhi</a>,
<strong>Sunil Jaiswal</strong>,	
<a href="https://web.xidian.edu.cn/ldli/en/index.html">Leida Li</a>,
<a href="https://personal.ntu.edu.sg/wslin/Home.html">Weisi Lin</a>                                                  
                                                       
                                                        
                                                       
							
              <br>
              <em>IEEE Transactions on Multimedia, 2022</em>
              <br>
              <a href="https://ieeexplore.ieee.org/abstract/document/9891833">Paper</a>
              <p style="text-align:justify;"> In this work, we propose a new and efficient quality assessment algorithm based upon 
                the variation in the depth of 3D synthesized and reference views.</p>
             </td>
          </tr> 
					
	

          <tr onmouseout="hypernerf_stop()" onmouseover="hypernerf_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" >
                <img src='images/gcpr.png' width="190">
              </div>
             </td>

               <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://link.springer.com/chapter/10.1007/978-3-030-92659-5_20">
                <papertitle>A Comparative Study of PnP and Learning Approaches to Super-Resolution in a Real-World Setting </papertitle>
              </a>
              <br>
							<a>Samim Zahoor Taray</a>, 
              <strong>Sunil Jaiswal</strong>,	
              <a>Shivam Sharma</a>,              
<a href="https://people.mpi-inf.mpg.de/~ncheema/">Noshaba Cheema</a>,
<a href="https://www.researchgate.net/profile/Klaus-Illgner">Klaus Illgner-Fehns</a>,
<a href="https://graphics.cg.uni-saarland.de/people/slusallek.html">Philipp Slusallek</a>,
<a href="https://www.cse.eti.uni-siegen.de/ivo-ihrke/">Ivo Ihrke</a>
              <br>
              <em>GCPR'2021</em>
              <br>
              <a href="https://link.springer.com/chapter/10.1007/978-3-030-92659-5_20">Paper</a>
              <p style="text-align:justify;"> The applicability to real-world images is limited
                and expectations are often disappointed when comparing to the performance
                achieved on synthetic data. For improving on this aspect, we investigate and
                compare two extensions of orthogonal popular techniques, namely plug-and-play
                optimization with learned priors, and a single end-to-end deep neural network
                trained on a larger variation of realistic synthesized training data, and compare
                their performance with special emphasis on model violations. We observe that the
                end-to-end network achieves a higher robustness and flexibility than the optimiza-
                tion based technique.
             </p>
             </td>
          </tr> 




          <tr onmouseout="hypernerf_stop()" onmouseover="hypernerf_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" >
                <img src='images/spie.png' width="190", height="110">
              </div>
             </td>

               <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://link.springer.com/chapter/10.1007/978-3-030-92659-5_20">
                <papertitle>Lightfield imaging for industrial applications.
                </papertitle>
              </a>
              <br>
              <a href="https://www.researchgate.net/profile/Klaus-Illgner">Klaus Illgner-Fehns</a>,
              <a>John Restrepo</a>, 
              <strong>Sunil Jaiswal</strong>,	
<a href="https://www.cse.eti.uni-siegen.de/ivo-ihrke/">Ivo Ihrke</a>
              <br>
              <em>SPIE Future Sensing Technologies, 2020</em>
              <br>
              <a href="https://link.springer.com/chapter/10.1007/978-3-030-92659-5_20">Paper</a>
              <p style="text-align:justify;"> 
                Lightfield imaging systems were brought to market for consumer and professional media recording. But so far, 
                this technology is less known for applications in the industrial space. The unique optical concept developed by KLens 
                allows to capture multiple perspectives of a scene with a single exposure as regular colour images on the camera sensor. 
             </p>
             </td>
          </tr> 
          
         
          
          <tr onmouseout="hypernerf_stop()" onmouseover="hypernerf_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" >
                <img src='' width="190", height="110">
              </div>
             </td>

               <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://ieeexplore.ieee.org/abstract/document/8960520">
                <papertitle>Kernel-ridge regression-based quality measure and enhancement of three-dimensional-synthesized images.
                </papertitle>
              </a>
              <br>
              <a href="https://sites.google.com/view/vinitjakhetiya/home?authuser=0">Vinit Jakhetiya</a>,
              <a>Ke Gu</a>, 
              <strong>Sunil P Jaiswal</strong>,	
<a>Trisha Singhal</a>,
<a>Zhifang Xia</a>
              <br>
              <em>IEEE Transactions on Industrial Electronics, 2020</em>
              <br>
              <a href="https://ieeexplore.ieee.org/abstract/document/8960520">Paper</a>
              <p style="text-align:justify;"> 
                In this article, we propose an efficient joint image quality assessment and enhancement algorithm for the 3-D-synthesized images 
                using a global predictor, namely, kernel ridge regression (KRR). 
             </p>
             </td>
          </tr> 




          <tr onmouseout="hypernerf_stop()" onmouseover="hypernerf_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" >
                <img src='' width="190", height="110">
              </div>
             </td>

               <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://ieeexplore.ieee.org/abstract/document/8049467">
                <papertitle>A Prediction Backed Model for Quality Assessment of Screen Content and 3D synthesized Images.
                </papertitle>
              </a>
              <br>
              <a href="https://sites.google.com/view/vinitjakhetiya/home?authuser=0">Vinit Jakhetiya</a>,
              <a>Gu Ke</a>,
              <a>Weisi Lin</a>, 
              <a>Qiaohong Li</a>,
              <strong>Sunil Jaiswal</strong>

              <br>
              <em>IEEE Transactions on Industrial Informatics, 2020</em>
              <br>
              <a href="https://ieeexplore.ieee.org/abstract/document/8049467">Paper</a>
              <p style="text-align:justify;"> 
                In this paper, we address problems associated with free-energy-principle-based image quality assessment (IQA) algorithms 
                for objectively assessing the quality of Screen Content (SC) and three-dimensional (3-D) synthesized images and also 
                propose a very fast and efficient IQA algorithm to address these issues.
             </p>
             </td>
          </tr> 
  
          

                              
          <tr onmouseout="hypernerf_stop()" onmouseover="hypernerf_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" >
                <img src='images/dcc.png' width="190", height="110">
              </div>
             </td>

               <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://ieeexplore.ieee.org/abstract/document/8712623">
                <papertitle>CNN-Based Driving of Block Partitioning for Intra Slices Encoding.
                </papertitle>
              </a>
              <br>
              <a>Franck Galpin</a>,
              <a>Fabien Racap√©</a>, 
              <strong>Sunil Jaiswal</strong>,	
<a>Philippe Bordes</a>,
<a>Fabrice Le L√©annec</a>
<a>Edouard Fran√ßois</a>

              <br>
              <em>Data Compression Conference (DCC), 2019</em>
              <br>
              <a href="https://ieeexplore.ieee.org/abstract/document/8712623">Paper</a>
              <p style="text-align:justify;"> 
                An encoding approach based on Convolutional Neural Networks is explored to partly substitute classical heuristics-based 
                encoder speed-ups by a systematic and automatic process. The solution allows controlling the trade-off between complexity 
                and coding gains, in intra slices, with one single parameter. 
             </p>
             </td>
          </tr> 



          


                    
          <tr onmouseout="hypernerf_stop()" onmouseover="hypernerf_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" >
                <img src='' width="190", height="110">
              </div>
             </td>

               <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://www.sciencedirect.com/science/article/abs/pii/S0925231217314522">
                <papertitle>Just noticeable difference for natural images using RMS contrast and feed-back mechanism.
                </papertitle>
              </a>
              <br>
              <a href="https://sites.google.com/view/vinitjakhetiya/home?authuser=0">Vinit Jakhetiya</a>,
              <a>Weisi Lin</a>, 
              <strong>Sunil Jaiswal</strong>,	
              <a>Gu Ke</a>,
              <a>Sharath C. Guntuku</a>
              <br>
              <em>Elsevier Neurocomputing, 2018</em>
              <br>
              <a href="https://www.sciencedirect.com/science/article/abs/pii/S0925231217314522">Paper</a>
              <p style="text-align:justify;"> 
              We propose the first pixel-based JND algorithm that includes a very important component of the human vision, namely CS by 
              measuring RMS contrast. This RMS contrast is combined with LA and CM to form a comprehensive pixel-domain model to efficiently 
              estimate JND in the low frequency regions. For high frequency regions (edge and texture), a feedback mechanism is proposed to 
              efficiently alleviate the over- and under-estimation of CM.
             </p>
             </td>
          </tr> 


          <tr onmouseout="hypernerf_stop()" onmouseover="hypernerf_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" >
                <img src='images/msfa.png' width="170", height="60">
              </div>
             </td>

               <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://ieeexplore.ieee.org/abstract/document/7762719">
                <papertitle>Adaptive Multispectral Demosaicking Based on Frequency Domain Analysis of Spectral Correlation.
                </papertitle>
              </a>
              <br>                           
              <strong>Sunil Jaiswal</strong>,	
              <a href="http://www.luvision.net/">Lu Fang</a>, 
              <a href="https://sites.google.com/view/vinitjakhetiya/home?authuser=0">Vinit Jakhetiya</a>, 
              <a>Jiahao Pang</a>,
              <a href="https://www3.cs.stonybrook.edu/~mueller/">Klaus Mueller</a>,
              <a href="https://www.linkedin.com/in/oscar-au-59b3a411/">Oscar Au</a>
              <br>
              <em>IEEE Transactions on Image Processing, 2017</em>
              <br>
              <a href="https://ieeexplore.ieee.org/abstract/document/7762719">Paper</a>
              <p style="text-align:justify;"> 
              Color filter array (CFA) interpolation, or three-band demosaicking, is a process of interpolating the missing color samples 
              in each band to reconstruct a full color image. In this paper, we are concerned with the challenging problem of multispectral 
              demosaicking, where each band is significantly undersampled due to the increment in the number of bands.
             </p>
             </td>
          </tr> 


          <tr onmouseout="hypernerf_stop()" onmouseover="hypernerf_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" >
                <img src='' width="190", height="110">
              </div>
             </td>

               <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://ieeexplore.ieee.org/document/7567576">
                <papertitle>Maximum a Posterior and Perceptually Motivated Reconstruction Algorithm: A Generic Framework.
                </papertitle>
              </a>
              <br>  
              <a href="https://sites.google.com/view/vinitjakhetiya/home?authuser=0">Vinit Jakhetiya</a>,   
              <a href="https://personal.ntu.edu.sg/wslin/Home.html">Weisi Lin</a>,                      
              <strong>Sunil Jaiswal</strong>,	
              <a href="https://sharathg.cis.upenn.edu//">Sharath C Guntuku</a>
              <a href="https://www.linkedin.com/in/oscar-au-59b3a411/">Oscar Au</a>
              <br>
              <em>IEEE Transactions on Multimedia, 2017</em>
              <br>
              <a href="https://ieeexplore.ieee.org/document/7567576">Paper</a>
              <p style="text-align:justify;"> 
              In this paper, we propose an efficient perceptually motivated and maximum a posterior (MAP)-based generic framework for 
              image reconstruction. This can be applied to several image/video processing applications, where there is a necessity to 
              improve reconstruction accuracy and suppress visible artifacts, such as denoising, deinterlacing, interpolation, de-blocking 
              of Jpeg/Jpeg-2000, and demosaicing.
             </p>
             </td>
          </tr> 
          
  
                    
  
        </tbody></table>

      </tbody></table>

      <div id="parent">
      <script type="text/javascript" id="clstr_globe" src="//clustrmaps.com/globe.js?d=PAW_gOsDuJXHxCkTVgqM7ohN5BLMqFb2T6ai6IlA94k"></script>
      </div>         
      
              
                      <br>
      <p style="text-align:center;font-size:small;">
                     
                    </p>
      <p style="text-align:center;font-size:small;">
                    Last updated on January 11, 2024 | Thanks  <a href="https://jonbarron.info/"> Dr. Jonathan T. Barron</a> for this awesome template.
                    </p>
                  </td>
                </tr>
              </tbody></table>
            </td>
          </tr>
        </table>
      </body>
      
      </html>